# Q-Learning Battery Charging Agent ğŸ”‹ğŸ¤–

Ce projet implÃ©mente un **agent d'apprentissage par renforcement (Q-learning)** qui apprend Ã  **gÃ©rer la charge d'une batterie** de maniÃ¨re optimale.  
L'objectif est de maximiser la rÃ©compense totale tout en maintenant un comportement efficace Ã  long terme.

---

## ğŸš€ Objectif du projet

Le but de ce projet est de **simuler un environnement de charge de batterie** dans lequel un agent intelligent apprend Ã  dÃ©cider :
- Quand charger la batterie.
- Quand Ã©conomiser l'Ã©nergie.
- Comment maximiser la performance sur plusieurs Ã©pisodes.

---

## ğŸ§  Fonctionnement

L'agent apprend grÃ¢ce Ã  une approche **Q-Learning avec rÃ©seau de neurones (Q-Network)**.

### 1. Environnement (`BatteryEnv`)
Lâ€™environnement simule une batterie avec un niveau de charge (entre 0 et 100).  
Chaque action (charger ou ne rien faire) modifie ce niveau de charge et donne une **rÃ©compense** selon le comportement.

- **Ã‰tat (state)** : Niveau de charge de la batterie.
- **Actions (action)** :
  - `0` â†’ Ne rien faire.
  - `1` â†’ Charger.
- **RÃ©compense (reward)** :
  - Positive si lâ€™action est efficace (ex : maintenir la batterie dans une plage optimale).
  - NÃ©gative si lâ€™action est inefficace (ex : surcharge ou sous-charge).

### 2. Agent Q-Learning (`qnet`)
Le rÃ©seau de neurones `qnet` estime les valeurs Q (Q-values) pour chaque action possible dans un Ã©tat donnÃ©.

Formule de mise Ã  jour :
```
Q(s, a) â† Q(s, a) + Î± [r + Î³ * max(Q(s', a')) âˆ’ Q(s, a)]
```
- `Î±` : Taux dâ€™apprentissage (learning rate).
- `Î³` : Facteur de rÃ©compense future (discount factor).

### 3. EntraÃ®nement
Le fichier `train_qlearning()` entraÃ®ne lâ€™agent sur plusieurs Ã©pisodes.  
Lâ€™agent explore dâ€™abord (grÃ¢ce Ã  `epsilon`), puis exploite ce quâ€™il a appris (rÃ©duction progressive dâ€™`epsilon`).

---

## ğŸ“ˆ Visualisation des rÃ©sultats

Ã€ la fin de lâ€™entraÃ®nement, une courbe montre la **rÃ©compense totale par Ã©pisode**, permettant dâ€™observer la progression de lâ€™agent.

```python
plt.plot(range(1, len(rewards) + 1), rewards)
plt.xlabel("Ã‰pisodes")
plt.ylabel("RÃ©compense totale")
plt.title("Progression de l'apprentissage")
plt.show()
```

---

## ğŸ§© Structure du projet

```
ğŸ“‚ QLearning-Battery
â”œâ”€â”€ qnet_model.pth          # Poids du modÃ¨le sauvegardÃ©
â”œâ”€â”€ main.py                 # Point d'entrÃ©e principal
â”œâ”€â”€ train.py                # Fonction d'entraÃ®nement Q-learning
â”œâ”€â”€ env.py                  # DÃ©finition de l'environnement BatteryEnv
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â””â”€â”€ README.md               # Ce fichier ğŸ“˜
```

---

## ğŸ› ï¸ Technologies utilisÃ©es

- **Python 3**
- **PyTorch** (rÃ©seau de neurones)
- **Matplotlib** (visualisation)
- **Numpy**

---

## ğŸ“Š Observations

- Le modÃ¨le apprend Ã  stabiliser sa stratÃ©gie de charge aprÃ¨s plusieurs Ã©pisodes.  
- Il peut parfois prÃ©senter des **fluctuations** (rÃ©compenses en baisse temporaire) dues Ã  :
  - Lâ€™exploration (`epsilon`).
  - La nature stochastique de lâ€™environnement.
- Plus le nombre dâ€™Ã©pisodes augmente, plus le comportement devient stable.

---

## ğŸ”„ AmÃ©liorations possibles

- Ajouter une mÃ©moire de rejouage (Experience Replay).
- ImplÃ©menter une version **Deep Q-Learning (DQN)**.
- Introduire un **epsilon dynamique** pour Ã©quilibrer exploration/exploitation.
- Simuler plusieurs batteries ou diffÃ©rents environnements de charge.

---

## ğŸ‘¨â€ğŸ’» Auteur
Projet rÃ©alisÃ© par **Ilyes Omri**, Ã©tudiant en informatique passionnÃ© par lâ€™intelligence artificielle et lâ€™apprentissage par renforcement.

