
# âš¡ Gestion dâ€™Ã©nergie avec Apprentissage par Renforcement (Q-Learning)

## ğŸ§  Description du projet
Ce projet met en Å“uvre un **systÃ¨me dâ€™apprentissage par renforcement (RL)** simple pour optimiser lâ€™utilisation de deux batteries alimentant un moteur Ã©lectrique.  
Lâ€™objectif est dâ€™apprendre une **politique de sÃ©lection de batterie** (A ou B) qui maximise la rÃ©compense en maintenant un Ã©quilibre entre leurs niveaux de charge tout en rÃ©pondant Ã  la demande Ã©nergÃ©tique du moteur.

Le projet combine :
- **Une ontologie RDF (Web sÃ©mantique)** pour reprÃ©senter les composants Ã©nergÃ©tiques (batteries, moteur).
- **Un environnement RL personnalisÃ©** (`EnergyEnv`) basÃ© sur cette ontologie.
- **Un agent dâ€™apprentissage Q-Learning** implÃ©mentÃ© avec un **rÃ©seau de neurones (PyTorch)**.

---

## ğŸ—‚ï¸ Structure du projet

```
â”œâ”€â”€ EnergyEnv.py         # Environnement RL basÃ© sur lâ€™ontologie
â”œâ”€â”€ ontology.py          # CrÃ©ation et gestion de lâ€™ontologie RDF
â”œâ”€â”€ main.py              # EntraÃ®nement du modÃ¨le avec Q-Learning
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ontology.ttl     # Fichier gÃ©nÃ©rÃ© contenant lâ€™ontologie RDF
â””â”€â”€ README.md            # Description du projet
```

---

## âš™ï¸ Fonctionnement

### 1ï¸âƒ£ Ontologie RDF
Le fichier `ontology.py` dÃ©finit une ontologie simple :
- Deux batteries (`BatteryA` et `BatteryB`)
- Un moteur (`Motor`)
- Des propriÃ©tÃ©s :
  - `hasCharge` â†’ niveau de charge de chaque batterie
  - `powerDemand` â†’ demande du moteur

```python
BatteryA : hasCharge = 80
BatteryB : hasCharge = 40
Motor    : powerDemand = 50
```

Le graphe RDF est sauvegardÃ© sous `data/ontology.ttl`.

---

### 2ï¸âƒ£ Environnement RL â€” `EnergyEnv`
Lâ€™environnement lit les donnÃ©es depuis lâ€™ontologie RDF et dÃ©finit :
- **Ã‰tat (state)** : `[chargeA, chargeB]`
- **Actions (action)** :
  - `0` â†’ utiliser la batterie A
  - `1` â†’ utiliser la batterie B
- **RÃ©compense (reward)** :
  - +10 points pour une action valide  
  - pÃ©nalitÃ© proportionnelle Ã  la diffÃ©rence de charge entre les batteries  
  - -10 si une batterie tombe Ã  zÃ©ro

Le but de lâ€™agent est dâ€™Ã©quilibrer les charges pour Ã©viter quâ€™une batterie ne soit totalement dÃ©chargÃ©e.

---

### 3ï¸âƒ£ Agent RL â€” `main.py`
Le fichier `main.py` implÃ©mente un agent **Q-Learning** Ã  lâ€™aide dâ€™un petit **rÃ©seau de neurones** :

- **EntrÃ©e :** 2 (charges des batteries)  
- **Sortie :** 2 (actions possibles)
- **RÃ©seau :**
  ```python
  nn.Linear(2, 32) â†’ ReLU â†’ nn.Linear(32, 2)
  ```
- **Optimiseur :** Adam (lr = 0.01)
- **Fonction de perte :** MSE
- **Facteur de discount :** Î³ = 0.9
- **Exploration :** Îµ-dÃ©croissant (de 0.1 Ã  0.01)

Lâ€™agent choisit quelle batterie utiliser pour maximiser la rÃ©compense cumulÃ©e.

---

## ğŸ“Š RÃ©sultats et Observations

- Le graphique final montre lâ€™Ã©volution du **total reward** au fil des Ã©pisodes.
- Le modÃ¨le apprend progressivement Ã  **Ã©quilibrer lâ€™utilisation des deux batteries**.
- Une **dÃ©croissance de Îµ** permet Ã  lâ€™agent de passer de lâ€™exploration Ã  lâ€™exploitation.
- Si une batterie tombe Ã  zÃ©ro, la pÃ©nalitÃ© de -10 encourage une gestion plus Ã©quilibrÃ©e.
- Le modÃ¨le entraÃ®nÃ© est sauvegardÃ© dans `qnet_model.pth` pour un futur rÃ©entraÃ®nement.

---

## ğŸš€ Lancer le projet

### 1ï¸âƒ£ Installer les dÃ©pendances
```bash
pip install -r requirements.txt 
```

### 2ï¸âƒ£ GÃ©nÃ©rer lâ€™ontologie
```bash
python ontology.py
```

### 3ï¸âƒ£ Lancer lâ€™entraÃ®nement
```bash
python main.py
```

---

## ğŸ§© Technologies utilisÃ©es
- **Python**
- **PyTorch** â€” pour le rÃ©seau de neurones du Q-Learning
- **RDFLib** â€” pour manipuler lâ€™ontologie
- **Matplotlib** â€” pour visualiser la courbe de rÃ©compenses
- **NumPy** â€” pour gÃ©rer les Ã©tats numÃ©riques

---

## ğŸ’¡ AmÃ©liorations possibles
- Ajouter un **systÃ¨me de recharge automatique** des batteries.
- Utiliser **DQN (Deep Q-Network)** avec mÃ©moire dâ€™expÃ©rience (replay buffer).
- Introduire plus de variables dans lâ€™ontologie (tempÃ©rature, rendement, etc.).
- Comparer les performances entre **Q-Learning tabulaire** et **rÃ©seau de neurones**.
